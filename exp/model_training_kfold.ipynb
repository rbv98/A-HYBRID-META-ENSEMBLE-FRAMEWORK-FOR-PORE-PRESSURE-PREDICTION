{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb2c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, KFold\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model, clone_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, MaxPooling1D,\n",
    "    Dropout, Flatten, Dense, LSTM, GRU\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, anderson_ksamp\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ad7e0b-e65e-4aab-86d5-bc4f87dcc848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d54f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tvd',\n",
       " 'dt',\n",
       " 'dt_nct',\n",
       " 'gr',\n",
       " 'phie',\n",
       " 'hp',\n",
       " 'ob',\n",
       " 'rhob_combined',\n",
       " 'res_deep',\n",
       " 'temp',\n",
       " 'vp',\n",
       " 'eaton_ratio',\n",
       " 'hp_gradient',\n",
       " 'ob_gradient',\n",
       " 'tvd_normalized']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('preprocessing_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "features = metadata['predictor_features']\n",
    "\n",
    "target_col = 'ppp'\n",
    "depth_col = 'tvd'\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a88ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data sizes:\n",
      "  Train+Val combined: 182238 (for CV stacking)\n",
      "  Test: 84930 (for final evaluation)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_data.csv')\n",
    "val_df = pd.read_csv('val_data.csv')\n",
    "train_val_combined = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "blind_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "X_train_val = train_val_combined[features].values\n",
    "y_train_val = train_val_combined[target_col].values\n",
    "\n",
    "X_test = blind_df[features].values\n",
    "y_test = blind_df[target_col].values\n",
    "\n",
    "print(f\"\\nData sizes:\")\n",
    "print(f\"  Train+Val combined: {len(X_train_val)} (for CV stacking)\")\n",
    "print(f\"  Test: {len(X_test)} (for final evaluation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645a7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21061b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: GENERATING OUT-OF-FOLD PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Performing 5-fold cross-validation...\n",
      "\n",
      "Fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761343330.005721 2918472 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 140842 MB memory:  -> device: 0, name: NVIDIA H200, pci bus id: 0000:bb:00.0, compute capability: 9.0\n",
      "2025-10-24 18:02:13.651135: I external/local_xla/xla/service/service.cc:163] XLA service 0x1482b0014480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-24 18:02:13.651158: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA H200, Compute Capability 9.0\n",
      "2025-10-24 18:02:13.761408: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-24 18:02:14.169242: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91400\n",
      "2025-10-24 18:02:14.422308: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:02:14.422340: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:02:14.422375: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:02:14.422387: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:02:14.422403: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:02:15.189786: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1580', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:15.372568: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2069', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:15.443190: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2868', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:15.764652: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2069', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:16.800977: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2868', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "I0000 00:00:1761343341.355600 4084875 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-10-24 18:02:29.379176: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:02:29.969509: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:30.999201: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:02:30.999229: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:02:31.216236: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:31.695748: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:31.807003: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:32.089783: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:32.099425: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-10-24 18:02:32.166449: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-10-24 18:03:26.438327: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:03:26.438391: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:03:26.438412: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:03:26.769911: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_722', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-24 18:03:27.405804: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_722', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-10-24 18:03:27.825737: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1901', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2025-10-24 18:03:28.290646: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1808', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-10-24 18:03:30.034801: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_select_fusion', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-24 18:03:34.589992: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-24 18:03:35.191097: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-10-24 18:03:35.216217: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-10-24 18:03:35.300867: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 18:15:19.267122: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_select_fusion', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: GENERATING OUT-OF-FOLD PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for out-of-fold predictions\n",
    "oof_preds = {\n",
    "    'CNN': np.zeros(len(X_train_val)),\n",
    "    'DFNN': np.zeros(len(X_train_val)),\n",
    "    'RNN': np.zeros(len(X_train_val)),\n",
    "    'RF': np.zeros(len(X_train_val)),\n",
    "    'XGBoost': np.zeros(len(X_train_val))\n",
    "}\n",
    "\n",
    "# Storage for final models (trained on all data)\n",
    "final_models = {}\n",
    "\n",
    "print(f\"\\nPerforming {n_folds}-fold cross-validation...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_folds}...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_fold_train = X_train_val[train_idx]\n",
    "    y_fold_train = y_train_val[train_idx]\n",
    "    X_fold_val = X_train_val[val_idx]\n",
    "    y_fold_val = y_train_val[val_idx]\n",
    "    \n",
    "    # Scale\n",
    "    fold_scaler = StandardScaler()\n",
    "    X_fold_train_s = fold_scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_s = fold_scaler.transform(X_fold_val)\n",
    "    \n",
    "    # Reshape for CNN/RNN\n",
    "    X_fold_train_cnn = X_fold_train_s.reshape(-1, len(features), 1)\n",
    "    X_fold_val_cnn = X_fold_val_s.reshape(-1, len(features), 1)\n",
    "    X_fold_train_rnn = X_fold_train_s.reshape(-1, len(features), 1)\n",
    "    X_fold_val_rnn = X_fold_val_s.reshape(-1, len(features), 1)\n",
    "    \n",
    "    # Train CNN\n",
    "    cnn_fold = Sequential([\n",
    "        Input((len(features),1)),\n",
    "        Conv1D(64, kernel_size=2, padding='same', activation='relu',\n",
    "               kernel_regularizer=regularizers.L2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.25),\n",
    "        Conv1D(128, kernel_size=2, padding='same', activation='relu',\n",
    "               kernel_regularizer=regularizers.L2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.002)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.L2(0.002)),\n",
    "        BatchNormalization(), \n",
    "        Dropout(0.3),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    cnn_fold.compile(optimizer='adam', loss='mse')\n",
    "    cnn_fold.fit(X_fold_train_cnn, y_fold_train,\n",
    "                 validation_split=0.1, epochs=100, batch_size=32,\n",
    "                 callbacks=[es, rlr], verbose=0)\n",
    "    oof_preds['CNN'][val_idx] = cnn_fold.predict(X_fold_val_cnn, verbose=0).flatten()\n",
    "    \n",
    "    # Train DFNN\n",
    "    dfnn_fold = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(len(features),)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.15),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.15),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    dfnn_fold.compile(optimizer='adam', loss='mse')\n",
    "    dfnn_fold.fit(X_fold_train_s, y_fold_train,\n",
    "                  validation_split=0.1, epochs=100, batch_size=64,\n",
    "                  callbacks=[es, rlr], verbose=0)\n",
    "    oof_preds['DFNN'][val_idx] = dfnn_fold.predict(X_fold_val_s, verbose=0).flatten()\n",
    "    \n",
    "    # Train RNN\n",
    "    rnn_fold = Sequential([\n",
    "        Input((len(features), 1)),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    rnn_fold.compile(optimizer='adam', loss='mse')\n",
    "    rnn_fold.fit(X_fold_train_rnn, y_fold_train,\n",
    "                 validation_split=0.1, epochs=130, batch_size=16,\n",
    "                 callbacks=[es, rlr], verbose=0)\n",
    "    oof_preds['RNN'][val_idx] = rnn_fold.predict(X_fold_val_rnn, verbose=0).flatten()\n",
    "    \n",
    "    # Train RF\n",
    "    rf_fold = RandomForestRegressor(\n",
    "        n_estimators=500, max_depth=15,\n",
    "        min_samples_split=5, min_samples_leaf=2,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf_fold.fit(X_fold_train_s, y_fold_train)\n",
    "    oof_preds['RF'][val_idx] = rf_fold.predict(X_fold_val_s)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_fold = xgb.XGBRegressor(\n",
    "        n_estimators=1000, max_depth=6,\n",
    "        learning_rate=0.1, subsample=0.8,\n",
    "        colsample_bytree=0.8, random_state=42,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    xgb_fold.fit(X_fold_train_s, y_fold_train,\n",
    "                 eval_set=[(X_fold_val_s, y_fold_val)],\n",
    "                 verbose=False)\n",
    "    oof_preds['XGBoost'][val_idx] = xgb_fold.predict(X_fold_val_s)\n",
    "\n",
    "print(\"\\n✓ Out-of-fold predictions generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: TRAINING META-MODEL ON OUT-OF-FOLD PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create feature matrix for meta-model\n",
    "X_meta = np.column_stack([\n",
    "    oof_preds['CNN'],\n",
    "    oof_preds['DFNN'],\n",
    "    oof_preds['RNN'],\n",
    "    oof_preds['RF'],\n",
    "    oof_preds['XGBoost']\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'CNN': cnn_fold,\n",
    "    'DFNN': dfnn_fold,\n",
    "    'RNN': rnn_fold,\n",
    "    'Random Forest': rf_fold,\n",
    "    'XGBoost': xgb_fold\n",
    "}\n",
    "\n",
    "# Train stacking model\n",
    "stacking_model = Ridge(alpha=1.0)\n",
    "stacking_model.fit(X_meta, y_train_val)\n",
    "\n",
    "print(\"\\nMeta-model trained!\")\n",
    "print(\"Ridge coefficients:\")\n",
    "for name, coef in zip(models, stacking_model.coef_):\n",
    "    print(f\"  {name:15}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57947f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: RETRAINING BASE MODELS ON ALL TRAIN+VAL DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale all data\n",
    "scaler_final = StandardScaler()\n",
    "X_train_val_s = scaler_final.fit_transform(X_train_val)\n",
    "X_test_s = scaler_final.transform(X_test)\n",
    "\n",
    "# Reshape\n",
    "X_train_val_cnn = X_train_val_s.reshape(-1, len(features), 1)\n",
    "X_test_cnn = X_test_s.reshape(-1, len(features), 1)\n",
    "X_train_val_rnn = X_train_val_s.reshape(-1, len(features), 1)\n",
    "X_test_rnn = X_test_s.reshape(-1, len(features), 1)\n",
    "\n",
    "print(\"Retraining all models on full dataset...\")\n",
    "\n",
    "# CNN\n",
    "print(\"  CNN...\")\n",
    "cnn_final = Sequential([\n",
    "    Input((len(features),1)),\n",
    "    Conv1D(64, kernel_size=2, padding='same', activation='relu',\n",
    "           kernel_regularizer=regularizers.L2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.25),\n",
    "    Conv1D(128, kernel_size=2, padding='same', activation='relu',\n",
    "           kernel_regularizer=regularizers.L2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.L2(0.002)),\n",
    "    BatchNormalization(), \n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "cnn_final.compile(optimizer='adam', loss='mse')\n",
    "cnn_final.fit(X_train_val_cnn, y_train_val,\n",
    "              validation_split=0.1, epochs=100, batch_size=32,\n",
    "              callbacks=[es, rlr], verbose=0)\n",
    "\n",
    "# DFNN\n",
    "print(\"  DFNN...\")\n",
    "dfnn_final = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(len(features),)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "dfnn_final.compile(optimizer='adam', loss='mse')\n",
    "dfnn_final.fit(X_train_val_s, y_train_val,\n",
    "               validation_split=0.1, epochs=100, batch_size=64,\n",
    "               callbacks=[es, rlr], verbose=0)\n",
    "\n",
    "# RNN\n",
    "print(\"  RNN...\")\n",
    "rnn_final = Sequential([\n",
    "    Input((len(features), 1)),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "rnn_final.compile(optimizer='adam', loss='mse')\n",
    "rnn_final.fit(X_train_val_rnn, y_train_val,\n",
    "              validation_split=0.1, epochs=130, batch_size=16,\n",
    "              callbacks=[es, rlr], verbose=0)\n",
    "\n",
    "# RF\n",
    "print(\"  Random Forest...\")\n",
    "rf_final = RandomForestRegressor(\n",
    "    n_estimators=500, max_depth=15,\n",
    "    min_samples_split=5, min_samples_leaf=2,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_final.fit(X_train_val_s, y_train_val)\n",
    "\n",
    "# XGBoost\n",
    "print(\"  XGBoost...\")\n",
    "xgb_final = xgb.XGBRegressor(\n",
    "    n_estimators=1000, max_depth=6,\n",
    "    learning_rate=0.1, subsample=0.8,\n",
    "    colsample_bytree=0.8, random_state=42\n",
    ")\n",
    "xgb_final.fit(X_train_val_s, y_train_val, verbose=False)\n",
    "\n",
    "print(\"\\n✓ All models retrained on full dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41781bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate(name, y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rel = rmse/(y_true.max()-y_true.min())*100\n",
    "    print(f\"{name}: R²={r2:.4f}, RMSE={rmse:.2f}, MAE={mae:.2f}, RelRMSE={rel:.2f}%\")\n",
    "\n",
    "y_test_pred_cnn = cnn_final.predict(X_test_cnn, verbose=0).flatten()\n",
    "y_test_pred_dfnn = dfnn_final.predict(X_test_s, verbose=0).flatten()\n",
    "y_test_pred_rnn = rnn_final.predict(X_test_rnn, verbose=0).flatten()\n",
    "y_test_pred_rf = rf_final.predict(X_test_s)\n",
    "y_test_pred_xgb = xgb_final.predict(X_test_s)\n",
    "\n",
    "print(\"\\nIndividual model results:\")\n",
    "evaluate(\"CNN\", y_test, y_test_pred_cnn)\n",
    "evaluate(\"DFNN\", y_test, y_test_pred_dfnn)\n",
    "evaluate(\"RNN\", y_test, y_test_pred_rnn)\n",
    "evaluate(\"Random Forest\", y_test, y_test_pred_rf)\n",
    "evaluate(\"XGBoost\", y_test, y_test_pred_xgb)\n",
    "\n",
    "# Stacking prediction\n",
    "X_test_meta = np.column_stack([\n",
    "    y_test_pred_cnn,\n",
    "    y_test_pred_dfnn,\n",
    "    y_test_pred_rnn,\n",
    "    y_test_pred_rf,\n",
    "    y_test_pred_xgb\n",
    "])\n",
    "y_test_pred_stacking = stacking_model.predict(X_test_meta)\n",
    "\n",
    "print(\"\\nEnsemble results:\")\n",
    "y_test_pred_simple = np.mean(X_test_meta, axis=1)\n",
    "evaluate(\"Simple Average\", y_test, y_test_pred_simple)\n",
    "evaluate(\"Stacking (CV-based, PROPER)\", y_test, y_test_pred_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2af85c-88b2-49e3-b293-eace13a002cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFlow",
   "language": "python",
   "name": "tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d90ef1",
   "metadata": {},
   "source": [
    "# Neural Network Models\n",
    "\n",
    "</br>\n",
    "\n",
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e240d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "NEURAL NETWORK MODELS FOR PORE PRESSURE\n",
      "==================================================\n",
      "Baseline to beat: CatBoost R² = 0.6132\n",
      "Best target from trees: overpressure\n",
      "Train samples: 153,638\n",
      "Val samples: 28,600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "val_df = pd.read_csv('val_data.csv')\n",
    "\n",
    "# Load metadata and tree model results\n",
    "with open('preprocessing_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "with open('tree_model_summary.pkl', 'rb') as f:\n",
    "    tree_results = pickle.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEURAL NETWORK MODELS FOR PORE PRESSURE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Baseline to beat: CatBoost R² = {tree_results['best_r2']:.4f}\")\n",
    "print(f\"Best target from trees: {tree_results['best_target']}\")\n",
    "print(f\"Train samples: {len(train_df):,}\")\n",
    "print(f\"Val samples: {len(val_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d20ec4",
   "metadata": {},
   "source": [
    "### Data Preparation for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec30fccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATA PREPARATION FOR NEURAL NETWORKS\n",
      "==================================================\n",
      "Feature shape: 15 features\n",
      "Training samples: 153638\n",
      "Validation samples: 28600\n",
      "\n",
      "Target statistics (overpressure):\n",
      "  Train: 758 ± 987 psi\n",
      "  Val: 330 ± 527 psi\n",
      "\n",
      "Pressure regime distribution:\n",
      "  Train - Under: 13.8%\n",
      "  Train - Normal: 21.8%\n",
      "  Train - Over: 64.4%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"DATA PREPARATION FOR NEURAL NETWORKS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get feature columns (excluding targets)\n",
    "feature_cols = [f for f in metadata['predictor_features'] \n",
    "                if f not in ['ppp', 'pressure_ratio', 'overpressure']]\n",
    "\n",
    "# Prepare features and targets\n",
    "X_train = train_df[feature_cols].values\n",
    "X_val = val_df[feature_cols].values\n",
    "\n",
    "# Try overpressure target (best from tree models)\n",
    "y_train_overpressure = (train_df['ppp'] - train_df['hp']).values\n",
    "y_val_overpressure = (val_df['ppp'] - val_df['hp']).values\n",
    "\n",
    "# Also prepare log(ppp) for comparison\n",
    "y_train_log = np.log1p(train_df['ppp'].values)\n",
    "y_val_log = np.log1p(val_df['ppp'].values)\n",
    "\n",
    "print(f\"Feature shape: {X_train.shape[1]} features\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "\n",
    "# Feature scaling - critical for neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Target scaling for overpressure\n",
    "target_scaler_op = StandardScaler()\n",
    "y_train_op_scaled = target_scaler_op.fit_transform(y_train_overpressure.reshape(-1, 1)).flatten()\n",
    "y_val_op_scaled = target_scaler_op.transform(y_val_overpressure.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"\\nTarget statistics (overpressure):\")\n",
    "print(f\"  Train: {y_train_overpressure.mean():.0f} ± {y_train_overpressure.std():.0f} psi\")\n",
    "print(f\"  Val: {y_val_overpressure.mean():.0f} ± {y_val_overpressure.std():.0f} psi\")\n",
    "\n",
    "# Check for pressure regimes\n",
    "pressure_ratio_train = train_df['ppp'] / train_df['hp']\n",
    "pressure_ratio_val = val_df['ppp'] / val_df['hp']\n",
    "\n",
    "print(f\"\\nPressure regime distribution:\")\n",
    "print(f\"  Train - Under: {(pressure_ratio_train < 0.9).mean()*100:.1f}%\")\n",
    "print(f\"  Train - Normal: {((pressure_ratio_train >= 0.9) & (pressure_ratio_train <= 1.1)).mean()*100:.1f}%\")\n",
    "print(f\"  Train - Over: {(pressure_ratio_train > 1.1).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f3ab5",
   "metadata": {},
   "source": [
    "### Build Deep Feedforward Neural Network (DFNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1266b14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DEEP FEEDFORWARD NEURAL NETWORK (DFNN)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │            \u001b[38;5;34m60\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">186,813</span> (729.74 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m186,813\u001b[0m (729.74 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">184,799</span> (721.87 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m184,799\u001b[0m (721.87 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,014</span> (7.87 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,014\u001b[0m (7.87 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Training DFNN...\n",
      "Epoch 1/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 2.4523 - mae: 0.5966 - val_loss: 1.7234 - val_mae: 0.3380 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1.4774 - mae: 0.3942 - val_loss: 1.1329 - val_mae: 0.3401 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.9115 - mae: 0.3652 - val_loss: 0.6934 - val_mae: 0.3419 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.5517 - mae: 0.3513 - val_loss: 0.4588 - val_mae: 0.3595 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.3638 - mae: 0.3440 - val_loss: 0.3757 - val_mae: 0.3896 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2808 - mae: 0.3390 - val_loss: 0.3780 - val_mae: 0.4361 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2468 - mae: 0.3347 - val_loss: 0.3959 - val_mae: 0.4760 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2350 - mae: 0.3326 - val_loss: 0.3230 - val_mae: 0.3947 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2287 - mae: 0.3303 - val_loss: 0.3781 - val_mae: 0.4565 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2266 - mae: 0.3292 - val_loss: 0.3449 - val_mae: 0.4300 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2242 - mae: 0.3274 - val_loss: 0.3273 - val_mae: 0.4060 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2246 - mae: 0.3283 - val_loss: 0.3405 - val_mae: 0.4239 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2229 - mae: 0.3266 - val_loss: 0.3603 - val_mae: 0.4392 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2214 - mae: 0.3263 - val_loss: 0.3490 - val_mae: 0.4349 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2219 - mae: 0.3263 - val_loss: 0.3576 - val_mae: 0.4441 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2206 - mae: 0.3253 - val_loss: 0.3581 - val_mae: 0.4394 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2199 - mae: 0.3254 - val_loss: 0.3555 - val_mae: 0.4347 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2177 - mae: 0.3246 - val_loss: 0.3376 - val_mae: 0.4315 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2051 - mae: 0.3171 - val_loss: 0.2991 - val_mae: 0.3870 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2001 - mae: 0.3154 - val_loss: 0.2981 - val_mae: 0.3901 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.2000 - mae: 0.3148 - val_loss: 0.3207 - val_mae: 0.4167 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1978 - mae: 0.3138 - val_loss: 0.3135 - val_mae: 0.4037 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1968 - mae: 0.3132 - val_loss: 0.3119 - val_mae: 0.4036 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1965 - mae: 0.3129 - val_loss: 0.2970 - val_mae: 0.3907 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1964 - mae: 0.3125 - val_loss: 0.3104 - val_mae: 0.3999 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1948 - mae: 0.3116 - val_loss: 0.3249 - val_mae: 0.4204 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1948 - mae: 0.3119 - val_loss: 0.3227 - val_mae: 0.4187 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1947 - mae: 0.3116 - val_loss: 0.2928 - val_mae: 0.3868 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1940 - mae: 0.3111 - val_loss: 0.3002 - val_mae: 0.3946 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1940 - mae: 0.3111 - val_loss: 0.2874 - val_mae: 0.3793 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1937 - mae: 0.3106 - val_loss: 0.3072 - val_mae: 0.3982 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1935 - mae: 0.3104 - val_loss: 0.3234 - val_mae: 0.4105 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1926 - mae: 0.3099 - val_loss: 0.3093 - val_mae: 0.3955 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1931 - mae: 0.3100 - val_loss: 0.3165 - val_mae: 0.4062 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1925 - mae: 0.3098 - val_loss: 0.3242 - val_mae: 0.4148 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1926 - mae: 0.3101 - val_loss: 0.3133 - val_mae: 0.4047 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1924 - mae: 0.3096 - val_loss: 0.3029 - val_mae: 0.3977 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1921 - mae: 0.3092 - val_loss: 0.3143 - val_mae: 0.4052 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1922 - mae: 0.3093 - val_loss: 0.3042 - val_mae: 0.3935 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1924 - mae: 0.3099 - val_loss: 0.2950 - val_mae: 0.3897 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1847 - mae: 0.3037 - val_loss: 0.3003 - val_mae: 0.3926 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1816 - mae: 0.3022 - val_loss: 0.3103 - val_mae: 0.4060 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1806 - mae: 0.3014 - val_loss: 0.3145 - val_mae: 0.4088 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1796 - mae: 0.3010 - val_loss: 0.3122 - val_mae: 0.4063 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1796 - mae: 0.3014 - val_loss: 0.3171 - val_mae: 0.4113 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1783 - mae: 0.3005 - val_loss: 0.3071 - val_mae: 0.4027 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 0.1781 - mae: 0.3005 - val_loss: 0.3157 - val_mae: 0.4061 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1786 - mae: 0.3009 - val_loss: 0.3115 - val_mae: 0.3974 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.1778 - mae: 0.3006 - val_loss: 0.3239 - val_mae: 0.4156 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1786 - mae: 0.3004 - val_loss: 0.3268 - val_mae: 0.4202 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1731 - mae: 0.2958 - val_loss: 0.3218 - val_mae: 0.4180 - learning_rate: 1.2500e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.1718 - mae: 0.2956 - val_loss: 0.3329 - val_mae: 0.4323 - learning_rate: 1.2500e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.1709 - mae: 0.2949 - val_loss: 0.3185 - val_mae: 0.4163 - learning_rate: 1.2500e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 0.1696 - mae: 0.2939 - val_loss: 0.3268 - val_mae: 0.4246 - learning_rate: 1.2500e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.1694 - mae: 0.2936 - val_loss: 0.3274 - val_mae: 0.4223 - learning_rate: 1.2500e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.1697 - mae: 0.2942 - val_loss: 0.3213 - val_mae: 0.4188 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 0.1689 - mae: 0.2935 - val_loss: 0.3240 - val_mae: 0.4181 - learning_rate: 1.2500e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.1685 - mae: 0.2935 - val_loss: 0.3196 - val_mae: 0.4146 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.1686 - mae: 0.2937 - val_loss: 0.3289 - val_mae: 0.4260 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 0.1681 - mae: 0.2934 - val_loss: 0.3175 - val_mae: 0.4148 - learning_rate: 1.2500e-04\n",
      "\n",
      "DFNN Results:\n",
      "  R²: 0.5439\n",
      "  RMSE: 510.4 psi\n",
      "  Underpressure R²: -1.8495\n",
      "\n",
      "Comparison:\n",
      "  CatBoost baseline: R² = 0.6132\n",
      "  DFNN: R² = 0.5439\n",
      "  Improvement: -6.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"DEEP FEEDFORWARD NEURAL NETWORK (DFNN)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def create_dfnn(input_dim, hidden_layers=[256, 128, 64, 32], \n",
    "                dropout_rate=0.3, l2_reg=0.01):\n",
    "    \"\"\"Create a deep feedforward network\"\"\"\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.BatchNormalization()\n",
    "    ])\n",
    "    \n",
    "    # Hidden layers with decreasing neurons\n",
    "    for i, units in enumerate(hidden_layers):\n",
    "        model.add(layers.Dense(\n",
    "            units, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(l2_reg),\n",
    "            kernel_initializer='he_normal'\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "dfnn_model = create_dfnn(\n",
    "    input_dim=X_train_scaled.shape[1],\n",
    "    hidden_layers=[512, 256, 128, 64, 32],\n",
    "    dropout_rate=0.3,\n",
    "    l2_reg=0.001\n",
    ")\n",
    "\n",
    "# Compile\n",
    "dfnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(dfnn_model.summary())\n",
    "\n",
    "# Callbacks\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining DFNN...\")\n",
    "history = dfnn_model.fit(\n",
    "    X_train_scaled, y_train_op_scaled,\n",
    "    validation_data=(X_val_scaled, y_val_op_scaled),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_scaled = dfnn_model.predict(X_val_scaled, verbose=0)\n",
    "y_pred_op = target_scaler_op.inverse_transform(y_pred_scaled)\n",
    "y_val_op = y_val_overpressure\n",
    "\n",
    "# Convert to PPP for evaluation\n",
    "ppp_pred = y_pred_op.flatten() + val_df['hp'].values\n",
    "ppp_true = val_df['ppp'].values\n",
    "\n",
    "dfnn_r2 = r2_score(ppp_true, ppp_pred)\n",
    "dfnn_rmse = np.sqrt(mean_squared_error(ppp_true, ppp_pred))\n",
    "\n",
    "print(f\"\\nDFNN Results:\")\n",
    "print(f\"  R²: {dfnn_r2:.4f}\")\n",
    "print(f\"  RMSE: {dfnn_rmse:.1f} psi\")\n",
    "\n",
    "# Check underpressure performance\n",
    "under_mask = pressure_ratio_val < 0.9\n",
    "if under_mask.sum() > 0:\n",
    "    under_r2 = r2_score(ppp_true[under_mask], ppp_pred[under_mask])\n",
    "    print(f\"  Underpressure R²: {under_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  CatBoost baseline: R² = {tree_results['best_r2']:.4f}\")\n",
    "print(f\"  DFNN: R² = {dfnn_r2:.4f}\")\n",
    "print(f\"  Improvement: {(dfnn_r2 - tree_results['best_r2'])*100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69d7b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DFNN IMPROVEMENTS - LAST ATTEMPT\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'Balkassar OXY 01'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GeoScience/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:235\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/GeoScience/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:174\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    173\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device\u001b[38;5;241m=\u001b[39mdevice(values))\n",
      "File \u001b[0;32m~/Documents/GeoScience/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device\u001b[38;5;241m=\u001b[39mdevice(values))\n",
      "File \u001b[0;32m~/Documents/GeoScience/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:167\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Balkassar OXY 01'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m     56\u001b[0m train_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwell_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(train_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwell_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 57\u001b[0m val_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwell_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_enhanced\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwell_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Add well as one-hot encoded features\u001b[39;00m\n\u001b[1;32m     60\u001b[0m well_one_hot_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(train_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwell_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m], prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwell\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GeoScience/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:134\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([])\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GeoScience/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:237\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'Balkassar OXY 01'"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"DFNN IMPROVEMENTS - LAST ATTEMPT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Add engineered features for DFNN\n",
    "def add_dfnn_features(df):\n",
    "    \"\"\"Add features that help DFNN without temporal leakage\"\"\"\n",
    "    df_feat = df.copy()\n",
    "    \n",
    "    # Depth-based polynomial features\n",
    "    df_feat['tvd_squared'] = df_feat['tvd'] ** 2\n",
    "    df_feat['tvd_log'] = np.log1p(df_feat['tvd'])\n",
    "    \n",
    "    # Interaction terms (domain knowledge)\n",
    "    df_feat['eaton_tvd'] = df_feat['eaton_ratio'] * df_feat['tvd_normalized']\n",
    "    df_feat['hp_ob_ratio'] = df_feat['hp'] / (df_feat['ob'] + 1e-6)\n",
    "    \n",
    "    # Binned depth regions (helps DFNN learn different behaviors at different depths)\n",
    "    df_feat['depth_bin'] = pd.cut(df_feat['tvd'], bins=10, labels=False)\n",
    "    \n",
    "    return df_feat\n",
    "\n",
    "# Apply feature engineering\n",
    "train_enhanced = add_dfnn_features(train_df)\n",
    "val_enhanced = add_dfnn_features(val_df)\n",
    "\n",
    "# 2. Try ensemble of DFNNs with different targets\n",
    "def create_specialized_dfnn(input_dim, architecture='deep'):\n",
    "    \"\"\"Different DFNN architectures\"\"\"\n",
    "    \n",
    "    if architecture == 'deep':\n",
    "        # Very deep network\n",
    "        hidden = [1024, 512, 256, 128, 64, 32, 16]\n",
    "    elif architecture == 'wide':\n",
    "        # Wide but shallow\n",
    "        hidden = [2048, 1024, 512]\n",
    "    else:  # 'funnel'\n",
    "        # Aggressive funnel\n",
    "        hidden = [512, 128, 32, 8]\n",
    "    \n",
    "    model = models.Sequential([layers.Input(shape=(input_dim,))])\n",
    "    \n",
    "    for units in hidden:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    return model\n",
    "\n",
    "# 3. Well-specific normalization (key insight from CatBoost)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode wells\n",
    "le = LabelEncoder()\n",
    "train_enhanced['well_encoded'] = le.fit_transform(train_enhanced['well_id'])\n",
    "val_enhanced['well_encoded'] = le.transform(val_enhanced['well_id'])\n",
    "\n",
    "# Add well as one-hot encoded features\n",
    "well_one_hot_train = pd.get_dummies(train_enhanced['well_encoded'], prefix='well')\n",
    "well_one_hot_val = pd.get_dummies(val_enhanced['well_encoded'], prefix='well')\n",
    "\n",
    "# Combine features\n",
    "feature_cols_enhanced = feature_cols + ['tvd_squared', 'tvd_log', 'eaton_tvd', \n",
    "                                        'hp_ob_ratio', 'depth_bin']\n",
    "X_train_enhanced = pd.concat([\n",
    "    train_enhanced[feature_cols_enhanced],\n",
    "    well_one_hot_train\n",
    "], axis=1).values\n",
    "\n",
    "X_val_enhanced = pd.concat([\n",
    "    val_enhanced[feature_cols_enhanced],\n",
    "    well_one_hot_val\n",
    "], axis=1).values\n",
    "\n",
    "# Scale\n",
    "scaler_enhanced = StandardScaler()\n",
    "X_train_scaled = scaler_enhanced.fit_transform(X_train_enhanced)\n",
    "X_val_scaled = scaler_enhanced.transform(X_val_enhanced)\n",
    "\n",
    "# Train improved DFNN\n",
    "print(f\"Enhanced features: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "dfnn_final = create_specialized_dfnn(X_train_scaled.shape[1], 'deep')\n",
    "dfnn_final.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='huber',  # More robust to outliers\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Use overpressure target\n",
    "y_train = train_enhanced['ppp'].values - train_enhanced['hp'].values\n",
    "y_val = val_enhanced['ppp'].values - val_enhanced['hp'].values\n",
    "\n",
    "# Train with better strategy\n",
    "history = dfnn_final.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=512,  # Larger batch for stability\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(patience=30, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(factor=0.5, patience=10)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = dfnn_final.predict(X_val_scaled, verbose=0).flatten()\n",
    "ppp_pred = y_pred + val_enhanced['hp'].values\n",
    "ppp_true = val_enhanced['ppp'].values\n",
    "\n",
    "final_r2 = r2_score(ppp_true, ppp_pred)\n",
    "print(f\"\\nImproved DFNN R²: {final_r2:.4f}\")\n",
    "print(f\"Original DFNN R²: 0.5458\")\n",
    "print(f\"CatBoost baseline: 0.6132\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d499369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
